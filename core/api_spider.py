# -*- coding: utf-8 -*-
"""
APIçˆ¬è™«å®ç°
"""

import requests
import time
from typing import Dict, List, Union
from urllib.parse import urljoin
import json 
from .base_spider import BaseSpider, SpiderConfig
from utils.cookie_loader import CookieLoader
from utils.logger import get_logger

logger = get_logger(__name__)


class ApiSpider(BaseSpider):
    """APIçˆ¬è™«ç±»"""
    
    def __init__(self, config: SpiderConfig):
        super().__init__(config)
        self.session = requests.Session()
        self._setup_session()


    def _setup_session(self):
        """è®¾ç½®è¯·æ±‚ä¼šè¯ - å¢å¼ºç‰ˆ"""
        # é»˜è®¤è¯·æ±‚å¤´
        default_headers = {
            'User-Agent': self.config.user_agent or (
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
                '(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            ),
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache',
            'x-requested-with': 'fetch'
        }
        
        # æ›´æ–°è‡ªå®šä¹‰è¯·æ±‚å¤´
        if self.config.headers:
            default_headers.update(self.config.headers)
        
        self.session.headers.update(default_headers)
        logger.info(f"è¯·æ±‚å¤´è®¾ç½®å®Œæˆ: {dict(list(self.session.headers.items())[:3])}...")
        
        # è®¾ç½®ä»£ç†
        if self.config.proxy:
            self.session.proxies = {'http': self.config.proxy, 'https': self.config.proxy}
            logger.info(f"ä»£ç†è®¾ç½®: {self.config.proxy}")
        
        # åŠ è½½cookies - å¢å¼ºç‰ˆ
        if self.config.cookies_file:
            cookie_loader = CookieLoader(self.config.cookies_file)
            cookies = cookie_loader.load()
            
            if cookies:
                # éªŒè¯cookies
                cookie_loader.validate_cookies(cookies)
                
                # æ·»åŠ cookiesåˆ°session
                for cookie in cookies:
                    self.session.cookies.set(cookie['name'], cookie['value'])
                
                # è®°å½•cookieä¿¡æ¯
                logger.info(f"å·²è®¾ç½® {len(cookies)} ä¸ªcookies")
                logger.info(f"å½“å‰åŸŸåcookies: {len(self.session.cookies)} ä¸ª")
                
                # æ˜¾ç¤ºå…³é”®cookie
                for cookie in cookies:
                    if cookie['name'] in ['z_c0', '_xsrf', 'd_c0']:
                        logger.info(f"ğŸª è®¾ç½®cookie: {cookie['name']}={cookie['value'][:20]}...")
            else:
                logger.warning("âš ï¸  æœªåŠ è½½åˆ°ä»»ä½•cookies")
        else:
            logger.info("æœªé…ç½®cookieæ–‡ä»¶")
    
    def _setup_session(self):
        """è®¾ç½®è¯·æ±‚ä¼šè¯"""
        # é»˜è®¤è¯·æ±‚å¤´
        default_headers = {
            'User-Agent': self.config.user_agent or (
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
                '(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            ),
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache'
        }
        
        # æ›´æ–°è‡ªå®šä¹‰è¯·æ±‚å¤´
        if self.config.headers:
            default_headers.update(self.config.headers)
        
        self.session.headers.update(default_headers)
        
        # è®¾ç½®ä»£ç†
        if self.config.proxy:
            self.session.proxies = {'http': self.config.proxy, 'https': self.config.proxy}
        
        # åŠ è½½cookies
        if self.config.cookies_file:
            cookie_loader = CookieLoader(self.config.cookies_file)
            cookies = cookie_loader.load()
            for cookie in cookies:
                self.session.cookies.set(cookie['name'], cookie['value'])
    
    def fetch_page(self, url: str, params: Dict = None) -> Dict:
        """è·å–é¡µé¢æ•°æ® - å¢å¼ºè°ƒè¯•ç‰ˆ"""
        try:
            logger.info(f"ğŸ“¡ è¯·æ±‚URL: {url}")
            if params:
                logger.debug(f"è¯·æ±‚å‚æ•°: {params}")
            
            # è®°å½•è¯·æ±‚å‰çš„cookies
            logger.debug(f"è¯·æ±‚å‰cookies: {len(self.session.cookies)} ä¸ª")
            for cookie in self.session.cookies:
                if cookie.name in ['z_c0', '_xsrf']:
                    logger.debug(f"è¯·æ±‚å‰cookie: {cookie.name}={cookie.value[:20]}...")
            
            response = self.session.get(
                url,
                params=params or self.config.params,
                timeout=self.config.timeout
            )
            response.raise_for_status()
            
            # è®°å½•å“åº”çŠ¶æ€
            logger.info(f"ğŸ“Š å“åº”çŠ¶æ€: {response.status_code}")
            logger.debug(f"å“åº”å¤´: {dict(list(response.headers.items())[:5])}")
            
            # å°è¯•è§£æJSON
            try:
                json_data = response.json()
                logger.debug(f"å“åº”æ•°æ®ç±»å‹: {type(json_data)}")
                
                # æ£€æŸ¥çŸ¥ä¹ç‰¹å®šçš„é”™è¯¯å“åº”
                if isinstance(json_data, dict):
                    if 'error' in json_data:
                        logger.error(f"APIé”™è¯¯: {json_data['error']}")
                        return {}
                    if json_data.get('code') != 200 and 'code' in json_data:
                        logger.error(f"ä¸šåŠ¡é”™è¯¯ç : {json_data.get('code')}, æ¶ˆæ¯: {json_data.get('message', '')}")
                        return {}
                
                return json_data
                
            except ValueError as e:
                logger.warning(f"å“åº”ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼: {e}")
                logger.debug(f"å“åº”å†…å®¹é¢„è§ˆ: {response.text[:200]}...")
                return {'html': response.text, 'status_code': response.status_code}
                
        except requests.exceptions.RequestException as e:
            logger.error(f"è¯·æ±‚å¤±è´¥ {url}: {e}")
            return {}
    
    def extract_list_data(self, data: Dict) -> List[Dict]:
        """æå–åˆ—è¡¨é¡µæ•°æ® - é€‚é…æ–°ç»“æ„"""
        if not self.config.list_page or 'fields' not in self.config.list_page:
            logger.warning("æœªé…ç½®åˆ—è¡¨é¡µå­—æ®µ")
            return []
        
        results = []
        items = data
        
        # è·å–æ•°æ®åˆ—è¡¨
        list_selector = self.config.list_page.get('list_selector', '')
        if list_selector:
            for path in list_selector.split('.'):
                if isinstance(items, dict) and path in items:
                    items = items[path]
                else:
                    items = []
                    break
        
        if not isinstance(items, list):
            items = [items] if items else []
        
        logger.info(f"æå–åˆ° {len(items)} ä¸ªåˆ—è¡¨é¡¹")
        
        # æå–å­—æ®µ
        fields = self.config.list_page.get('fields', [])
        for i, item in enumerate(items):
            record = {}
            
            for field_config in fields:
                field_name = field_config['name']
                selector = field_config.get('selector', '')
                
                value = self._extract_value(item, selector)
                record[field_name] = value
                
                # è°ƒè¯•ï¼šè®°å½•æå–ç»“æœ
                if value:
                    logger.debug(f"  æå–å­—æ®µ '{field_name}' = '{value[:50]}...'")
                else:
                    logger.debug(f"  å­—æ®µ '{field_name}' æå–ä¸ºç©º")
            
            if record:
                results.append(record)
                logger.debug(f"è®°å½• {i+1}: {list(record.keys())}")
        
        return results
    
    def _extract_value(self, data: Union[Dict, str], selector: str) -> str:
        """ä»æ•°æ®ä¸­æå–å€¼ - æœ€ç»ˆä¿®å¤ç‰ˆ"""
        if not selector:
            return ''
        
        if isinstance(data, dict):
            # æ”¯æŒç‚¹è¯­æ³•è®¿é—®åµŒå¥—å­—æ®µ
            keys = selector.split('.')
            value = data
            
            for key in keys:
                if isinstance(value, dict) and key in value:
                    value = value[key]
                else:
                    return ''
            
            # å¤„ç†ä¸åŒç±»å‹çš„å€¼
            if value is None:
                return ''
            elif isinstance(value, (str, int, float)):
                return str(value)
            elif isinstance(value, dict):
                # å¦‚æœå€¼æ˜¯dictï¼Œå°è¯•è·å–textå­—æ®µï¼ˆçŸ¥ä¹çš„æ–°æ ¼å¼ï¼‰
                if 'text' in value:
                    return str(value['text'])
                else:
                    return json.dumps(value, ensure_ascii=False)[:100]
            elif isinstance(value, list):
                return ','.join(map(str, value))
            else:
                return str(value)
        
        return ''
    
    def crawl_detail_page(self, url: str) -> Dict:
        """çˆ¬å–è¯¦æƒ…é¡µ"""
        try:
            data = self.fetch_page(url)
            detail_data = {}
            
            if self.config.detail_page and 'fields' in self.config.detail_page:
                for field_config in self.config.detail_page['fields']:
                    field_name = field_config['name']
                    selector = field_config.get('selector', '')
                    attribute = field_config.get('attribute', 'text')
                    
                    value = self._extract_value(data, selector, attribute)
                    detail_data[field_name] = value
            
            return detail_data
        except Exception as e:
            logger.error(f"çˆ¬å–è¯¦æƒ…é¡µå¤±è´¥ {url}: {e}")
            return {}
    
    def crawl(self) -> List[Dict]:
        """æ‰§è¡Œçˆ¬å–"""
        logger.info(f"å¼€å§‹APIçˆ¬è™«: {self.config.name}")
        self.validate_config()
        
        current_page = 1
        while current_page <= self.config.max_pages:
            logger.info(f"çˆ¬å–ç¬¬ {current_page} é¡µ")
            
            # æ„å»ºè¯·æ±‚å‚æ•°
            params = self.config.params.copy() if self.config.params else {}
            if self.config.pagination:
                page_param = self.config.pagination.get('param', 'page')
                page_size_param = self.config.pagination.get('size_param', 'size')
                page_size = self.config.pagination.get('size', 20)
                
                params[page_param] = current_page
                if page_size_param:
                    params[page_size_param] = page_size
            
            # è·å–æ•°æ®
            url = self.config.base_url
            data = self.fetch_page(url, params)
            
            if not data:
                break
            
            # æå–åˆ—è¡¨æ•°æ®
            list_data = self.extract_list_data(data)
            if not list_data:
                break
            
            # å¤„ç†è¯¦æƒ…é¡µ
            if self.config.detail_page:
                for item in list_data:
                    if '_detail_url' in item:
                        detail_data = self.crawl_detail_page(item['_detail_url'])
                        item.update(detail_data)
                        del item['_detail_url']  # åˆ é™¤ä¸´æ—¶å­—æ®µ
            
            self.results.extend(list_data)
            
            # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰ä¸‹ä¸€é¡µ
            if not self._has_next_page(data, current_page):
                break
            
            current_page += 1
            time.sleep(self.config.delay)
        
        logger.info(f"çˆ¬å–å®Œæˆï¼Œå…±è·å– {len(self.results)} æ¡æ•°æ®")
        return self.results
    
    def _has_next_page(self, data: Dict, current_page: int) -> bool:
        """æ£€æŸ¥æ˜¯å¦è¿˜æœ‰ä¸‹ä¸€é¡µ"""
        if not self.config.pagination:
            return False
        
        # ç®€å•çš„åˆ†é¡µé€»è¾‘ï¼Œå¯ä»¥æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
        return len(self.extract_list_data(data)) > 0